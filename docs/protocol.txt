Protocol - Generating and Consuming Tabular Data
================================================

Generating Tabular Data
-----------------------

Any object is a *tabular data source* if:

1. It is a container, i.e., it implements the `__iter__` method.

2. It supports repeated iteration, i.e., each call to iter(source)
   returns an iterator that yields the same sequence of objects.

3. Each object returned by the iterator is itself a sequence (e.g., a
   list or tuple, but not an iterator).

So, for example, a list of lists is a tabular data source::

    >>> table = [['foo', 'bar'], ['A', '1'], ['B', '2']]
    >>> for row in table: print row
    ['foo', 'bar']
    ['A', '1']
    ['B', '2']
    >>> for row in table: print row
    ['foo', 'bar']
    ['A', '1']
    ['B', '2']

However, a `csv.reader` object **may not** be a tabular data source,
e.g., if it backed by a file, because it may only be possible to
iterate over the data once. E.g.::

    >>> import StringIO
    >>> file = StringIO.StringIO("""foo,bar
    A, 1
    B, 2
    """)
    >>> import csv
    >>> table = csv.reader(file)
    >>> for row in table: print row
    ['foo', 'bar']
    ['A', '1']
    ['B', '2']
    >>> for row in table: print row

The main reason for requiring tabular data sources to support repeated
iteration is that some transformations need to make more than one pass
through the data.

Data sources backed by files can usually be wrapped to support
repeated iterations, e.g.::

    >>> class FileBackedCsvSource(object):
    ...     def __init__(self, path, *args, **kwargs):
    ...         self.path = path
    ...         self.args = args
    ...         self.kwargs = kwargs
    ...     def __iter__(self):
    ...         with open(self.path, 'r') as file:
    ...             reader = csv.reader(file, *self.args, **self.kwargs)
    ...             for row in reader:
    ...                 yield row

Note that requiring tabular data sources to support repeated iteration
does add some complexity for implementation of this protocol, because
data source classes need to take care to support repeated
iteration. Whether it is strictly necessary or even a good idea
depends on various factors that I haven't fully explored yet, such as
whether it makes sense to implement all transformers in a fully
streaming fashion, even when they need to make more than one pass over
the data, and whether there are other use cases that justify this
decision, such as scripting a transformation pipeline where you want
to iterate over the pipeline output multiple times (or whether it is
generally more sensible to persist any data you might want to iterate
over more than once to a file).

Consuming Tabular Data
----------------------

The following assumptions must be made by any object or function that
is consuming data from a tabular data source:

1. The first object returned by the iterator is a "header row", i.e.,
   a sequence of "field names". This is typically a sequence of
   strings, but could be a sequence of objects of any type. These
   objects are used to index (i.e., refer to) data values within each
   subsequent row, in addition to positional indexing.

2. All subsequent objects returned by the iterator are "data
   rows". Each data row is a sequence of objects of any type.

On Datatypes
------------

This protocol places no constraints on the type of objects that occur
as data values in rows obtained from tabular data sources. A table may
contain all strings or all integers, or all values in a particular
field may be integers or floats, or there may be no consistent pattern
in the use of datatypes within a table.

TODO
