To Do
=====

- [DONE] Change naming convention to use all lower case without any
  underscores: It's just a pain and slows you down, especially in an
  interactive session, if you have to think about capitalisation or
  underscores. Note that hardly any unix commands ever use
  capitalisation. Given the emphasis on ease of use for petl, I think
  this is justified, even though it's not strictly consistent with the
  Python style guide.

- [DONE] Change API for `rowlengths`: Accept a table, and optional
  limit on number of rows to examine, and return a table of lengths
  and counts (probably easiest to implement as a list of
  lists). E.g.::

    >>> table1 = ...
    >>> rowlengths(table1)
    (('length', 'count'), (3, 9), (2, 1))

- [DONE] Change API for `distinctvalues`: Change name to `values`,
  make this a callable, accept a table and a field name, and optional
  limit on number of rows to examine, and return a two column table
  where the first field is the values, and the second field is counts,
  sorted by most common (i.e., count descending). Returning a list of
  lists is probably the simplest, and could be implemented by a call
  to Counter.most_common() prepended with a pair of field
  names. E.g.::

    >>> table1 = ...
    >>> values(table1, 'foo')
    (('value', 'count'), ('A', 3), ('C', 1))

- [DONE] Change API for `datatypes`: Change name to `types` and make
  it callable, accepting a table and field name, and returning a two
  column table of types and counts, similar to `values`. E.g.::

    >>> table1 = ...
    >>> types(table1, 'foo')
    (('type', 'count'), (int, 3), (str, 2), (float, 1))

- [DONE] Implement `parsetypes` function as replacement to
  `DataTypes`: A function accepting a table and field name, an
  optional limit on number of rows to examine, and an optional list of
  parser functions, returning a table of types and counts. The default
  list of parser functions should be (int, float), define some
  additional functions for parsing dates and times based on a specific
  pattern, or falling back to a list of known patterns, but don't
  include by default as they are super slow (e.g., dateparser,
  timeparser). E.g.::

    >>> table1 = ...
    >>> parsetypes(table1, 'foo')
    (('type', 'count'), (int, 2), (float, 1)) 

- [DONE] Implement `stats` as replacement to `BasicStatistics`:
  Implement a `stats` function which accepts a table and a field name,
  and optionally a limit on the number of values to examine, and
  returns basic descriptive statistics as a dictionary. E.g.::

    >>> table1 = ...
    >>> stats(table1, 'foo')
    {'min': 1, 'max': 7, 'mean': 3.4, 'sum': 34, 'count': 10, 'errors': 2}

- [DONE] Clean up the profile module and tests.

- [DONE] Implement `rslice`: Like islice but slice data rows and
  always include the header row. (Can't use 'slice' because already
  used in Python.)

- [DONE] Implement `head`: Return a table with the first n
  rows. E.g.::

    >>> table1 = ...
    >>> t1head = head(table1) # default to 10?
    >>> t1head = head(table1, 20) # return first 20

- [DONE] Implement `tail`: Return a table with the last n rows. E.g.::

    >>> table1 = ...
    >>> t1tail = tail(table1) # default to 10?
    >>> t1tail = tail(table1, 20) # return last 20

- [DONE] Implement `count`: Simple function counting number of rows in
  a table. E.g.::

    >>> table1 = ...
    >>> count(table1) # N.B., count does not include header row (field names)
    10

- [DONE] Implement `fields`: Simple function returning list of fields
  in a table. E.g.::

    >>> table1 = ...
    >>> fields(table1)
    ['foo', 'bar', 'baz]

- [DONE] Implement `see`: Similar to `look` but view transposed, i.e.,
  fields arranged vertically. E.g.::

    >>> table1 = [['foo', 'bar'], ['A', 1], ['B', 2]]
    >>> see(table1)
    'foo': 'A', 'B'
    'bar': 1, 2

- [DONE] Review API for `convert`: Maybe change 'add' to 'set', and
  also implement __setitem__. Rationale is, 'add' is confusing, if
  called twice on the same field the second call will override the
  first.

- [DONE] Change API for `sort`, `filterduplicates`, `filterconflicts`,
  `mergeduplicates`: Make sure all these take a 'key' keyword
  argument, instead of specifying the key as a list of arguments, so
  there is consistency across these and `melt`, `recast` etc.

- [DONE[ Implement default `sort` without key: Change 'Sort' to
  `sort`. If no key is supplied to `sort`, make the default sort a
  lexical sort on rows, i.e., sort by field 1, then field 2, etc.

- [DONE] Implement `complement` - Subtract rows in one table from
  another table, and return a table of the remaining rows. Probably
  need to do a lexical sort on both tables first, then iterate. E.g.::

    >>> table1 = ...
    >>> table2 = ...
    >>> t1_minus_t2 = complement(table1, table2)

- [DONE] Implement `complementpresorted`: Same as `complement` but
  assume tables are already sorted.

- [DONE] Implement `diff` - Compare two tables, row by row, to find
  rows in one but not in the other. Probably need to do a lexical sort
  on the rows, then iterate and compare. What to do about each side of
  the difference?  Possible to return two tables, only in left and
  only in right? Maybe a dependency here on a complement function?
  E.g.::

    >>> table1 = ...
    >>> table2 = ...
    >>> added, subtracted = diff(table1, table2)
    >>> # added is rows found in table2 but not in table1
    >>> # subtracted is rows found in table1 but not in table2     

- [DONE] Implement `diffpresorted`: Same as `diff`, but assume tables
  are already sorted. This is closer to what the original diff on text
  files does, i.e., compare the files line by line as they are.

- [DONE] Implement `data`: Simple convenience function to return the
  data rows of a table.

- [DONE] Implement `valueset`: Convenience function to return values
  found in a column as a set (for easy access and comparison).

- [DONE] Implement `translate` (or can this be done with `convert`?):
  Convenience function to translate values in a single field.

- [DONE] Implement `rename`: Convenience function to rename one (or
  more?)  fields.

- [DONE] Implement `unique`: Simple test of uniqueness of a given
  field or selection of fields, returns true or false.

- [DONE] Implement `addfield`: Add a new field to a table, passing in
  a function to generate the values (possibly from existing values in
  the table, i.e., a calculated field).

- Rename `addfield` to `extend` for consistency with relational
  algebra.

- Implement `rmap`: A transformation mapping each row in the input to
  one row in the output, based on a single transformation
  function. E.g.::

    >>> table1 = ... # assume has fields 'foo', 'bar' and 'baz'
    >>> table2 = rmap(table1, fields=('new1', 'new2'), mapper=lambda r: [r['foo'] * 2, r['bar'] * r['baz']])
    >>> # N.B., mapper function will be passed a dictionary of values indexed by field, instead of raw rows

- Implement `fmap`: Like `rmap`, but there is one function for each
  field in the output. E.g.::

    >>> table1 = ... # assume has fields 'foo', 'bar' and 'baz'
    >>> table2 = fmap(table1)
    >>> # N.B., mapper functions will be passed a dictionary of values indexed by field, instead of raw rows
    >>> table2.set('new1', lambda r: r['foo'] * 2)
    >>> table2.set('new2', lambda r: r['bar'] * r['baz'])
    >>> table2.set('new3', 'foo') # copy of field 'foo'
    >>> table2.set('new4', '$bar * $baz') # evaluated expression
    >>> table2.set('new5', '${bar} * ${baz}') # evaluated expression with syntax to accommodate fields with spaces
    >>> translation = ((1, 'M'), (2, 'F'))
    >>> table2.set('new6', translate('foo', translation)) # convenience mapper function for translating values
    >>> # N.B., translation could also be a dictionary
    >>> table2['new7'] = '$foo + $bar' # can also use __setitem__
    >>> # N.B., use ordereddict under the hood, so fields come out in the order they were set

- Implement `filter`.

- Implement `filtererrors`, e.g.::

    >>> errs = filtererrors(tbl)
    >>> errs.add('foo', nullable(int, None))
    >>> look(errs)

- Implement `facet`.

- Implement `join` as equijoin if 'key' argument is supplied, or
  natural join if no 'key' argument given. Use sort-merge as default
  join algorithm. Also implement `joinpresorted`.

- Implement `outerjoin` as full outer join, and
  `outerjoinpresorted`. Also `leftjoin` and `rightjoin` with presorted
  versions.

- Implement `crossjoin` as simple cartesian product? Or is that just
  what happens with a natural join if there are no fields in common?

- Implement `thetajoin`? Or is this just a crossjoin followed by a
  filter?

- Join of diff?: Would be really useful to join the added and
  subtracted components of a diff on some (hopefully unique) key,
  which would give you a side-by-side comparison of the
  differences. Maybe doable with an appropriate join function, i.e.,
  requires do dedicated function?

- Implement additional signature to `values` function to only look at
  specific values (e.g., if you want to look at the level of
  missingness), and also output percentages, e.g.::

    >>> values(tbl, 'foo', {None})
    [['value', 'count', 'percent'], [None, 12, 32.7]]

- Implement `reduce`? Providing a single reduce function to reduce a
  list of rows into a single row.

- Implement `aggregate`? Providing an aggregation function for each
  non-key field.

- Implement `pivot`? (How is this different from `recast`?)

- Implement `transpose`.

- Implement `validator` (or maybe `validate`?).

- Implement `filtervalid` and `filterinvalid`. Or maybe
  `validator.filter`? Or maybe...

- Implement `fillup` and `filldown`.

- Implement `stringformat`, `stringreplace`, `stringtranslate`,
  `substringafter`, `substringbefore`, `stringsplitintorows`,
  `stringcaptureintorows`.

- Implement `recastpresorted`, ... and presorted equivalents for other
  functions that require sorted data.

- Implement merge sort with configurable buffer/chunk size, and with
  optional caching, on `sort`.

- Add skip argument to `readcsv` to cater for cases where there is
  bumf before the header row.

- Implement `readtxt` or `readlines`: Function to make a text file
  available as a simple single column table, where each row is a
  line. This or some other way to support working with files where you
  need to filter out lines you want to ignore, e.g., gff, or other
  cases where you need to preprocess before you can do normal csv
  parsing. Also possibly filter out based on row length (e.g., to deal
  with gff files with fasta at the end).

- Implement `hashtable`, i.e., index rows from a table by some key
  value, either using in memory dictionaries or the shelve
  module. N.B., this is similar to faceting.

- Implement joins using either sort-merge or hashtable, and give
  option to switch default implementation.

- Implement intersect? I.e., find rows in each table where a key value
  also exists in the other? Or is this just what you'd get with an
  equijoin? Or maybe intersect should be true intersection, i.e.,
  exact rows in common?

- Rename `cut` to `project` and `filter` to `select` for consistency
  with relational algebra? Or make them aliases? Or don't bother (cut
  and filter may be more intuitive).

- Implement `antijoin`? N.B., this is different from `complement`,
  because that looks at the whole row, whereas `antijoin` looks for
  absence of a key value.

- Change default reduce function in `recast` to return a list of all
  values, rather than just the first value arbitrarily.

- Change default `mergeduplicates` behaviour under conflicts,
  collecting conflicting values and output as a list? Or as an option?

(N.B., mergeduplicates is a special case of reduce/aggregate, ...)

- Some sort of mapreduce map function, i.e., yield a sequence of key,
  value pairs based on some table input?

- Add `stringjoin` as an aggregation function.

